,params,normalize,reward,training_time,mean_reward,std_reward,win_rate,loss_rate,draw_rate,model_type,policy,n_timesteps,run_name,how_many_to_add,best_agent_mean_reward,best_agent_std_reward
0,"{'buffer_size': 1000000, 'batch_size': 2048, 'gamma': 0.98, 'learning_rate': 0.00059, 'device': 'cuda', 'train_freq': 256, 'gradient_steps': 256, 'policy_kwargs': {'net_arch': [256, 256]}, 'tau': 0.08, 'policy_delay': 2, 'target_noise_clip': 0.3, 'policy': <class 'stable_baselines3.td3.policies.TD3Policy'>, 'env': <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x7f6479c036a0>, 'verbose': 1, 'tensorboard_log': '../final_hypersearch2', 'action_noise': OrnsteinUhlenbeckActionNoise(mu=[0. 0. 0. 0.], sigma=[0.38512363 0.38512363 0.38512363 0.38512363])}",True,0,0,-inf,0,0,0,0,TD3,<class 'stable_baselines3.td3.policies.TD3Policy'>,40000000,final_td3_h5_r01,5,-inf,0
