,params,normalize,reward,discrete_action_space,training_time,mean_reward,std_reward,win_rate,loss_rate,draw_rate,model_type,policy,n_timesteps,run_name,best_agent_mean_reward,best_agent_std_reward
0,"{'buffer_size': 1000000, 'batch_size': 2048, 'gamma': 0.99, 'learning_rate': 0.0009200000000000001, 'device': 'cpu', 'train_freq': 512, 'gradient_steps': 512, 'policy_kwargs': {'net_arch': [256, 256]}, 'tau': 0.02, 'policy_delay': 1, 'target_noise_clip': 0.1, 'policy': <class 'stable_baselines3.td3.policies.TD3Policy'>, 'env': <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x112740670>, 'verbose': 1, 'tensorboard_log': '../final_exp_2'}",True,0,False,3674.0228679180145,9.91,0,0.994,0.003,0.003,TD3,<class 'stable_baselines3.td3.policies.TD3Policy'>,20000000,td3_0_with_self,-inf,0
